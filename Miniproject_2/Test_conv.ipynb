{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d67a7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch\n",
    "from torch import empty , cat , arange\n",
    "from torch . nn . functional import fold , unfold\n",
    "\n",
    "#to delete\n",
    "#from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8d8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)\n",
    "\n",
    "inp = torch.randn(1, 3, 10, 12)\n",
    "w = torch.randn(2, 3, 4, 5)\n",
    "inp_unf = torch.nn.functional.unfold(inp, (4, 5))\n",
    "out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\n",
    "out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d290546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   \\nclass ConvolutionTransposed(object):\\n    def __init__(self, channels_input, channels_output, kernel_size, stride):\\n        self.weight = torch.empty(channels_output, channels_input, kernel_size, kernel_size).normal_()\\n        \\n        self.channel_input = channels_input\\n        #print('input channels', self.channel_input)\\n        self.kernel_size = kernel_size\\n        self.stride = stride\\n        \\n        print('weight',self.weight.shape)\\n        \\n        \\n    def forward(self, imgs):\\n        #print('forward')\\n        _,_,H,W = imgs.shape\\n        H_out = (H - 1)*self.stride + self.kernel_size \\n        W_out = (W - 1)*self.stride + self.kernel_size \\n        \\n        #print('Hout Wout', H_out, W_out)\\n        \\n        self.x = imgs.permute(1, 2, 3, 0).reshape(self.channel_input, -1)\\n        #print('x',self.x.shape)\\n        self.y = (self.weight.reshape(self.channel_input, -1)).t().matmul(self.x)\\n        #print('y',self.y.shape)\\n        self.y = self.y.reshape(self.y.shape[0], -1, imgs.shape[0])\\n        #print('y2',self.y.shape)\\n        self.y = self.y.permute(2, 0, 1)\\n        \\n        #print(self.y.shape)\\n        self.y = fold( self.y, (H_out, W_out), kernel_size=(self.kernel_size,self.kernel_size), stride=self.stride)\\n        \\n        return self.y\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"   \n",
    "class ConvolutionTransposed(object):\n",
    "    def __init__(self, channels_input, channels_output, kernel_size, stride):\n",
    "        self.weight = torch.empty(channels_output, channels_input, kernel_size, kernel_size).normal_()\n",
    "        \n",
    "        self.channel_input = channels_input\n",
    "        #print('input channels', self.channel_input)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        print('weight',self.weight.shape)\n",
    "        \n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        #print('forward')\n",
    "        _,_,H,W = imgs.shape\n",
    "        H_out = (H - 1)*self.stride + self.kernel_size \n",
    "        W_out = (W - 1)*self.stride + self.kernel_size \n",
    "        \n",
    "        #print('Hout Wout', H_out, W_out)\n",
    "        \n",
    "        self.x = imgs.permute(1, 2, 3, 0).reshape(self.channel_input, -1)\n",
    "        #print('x',self.x.shape)\n",
    "        self.y = (self.weight.reshape(self.channel_input, -1)).t().matmul(self.x)\n",
    "        #print('y',self.y.shape)\n",
    "        self.y = self.y.reshape(self.y.shape[0], -1, imgs.shape[0])\n",
    "        #print('y2',self.y.shape)\n",
    "        self.y = self.y.permute(2, 0, 1)\n",
    "        \n",
    "        #print(self.y.shape)\n",
    "        self.y = fold( self.y, (H_out, W_out), kernel_size=(self.kernel_size,self.kernel_size), stride=self.stride)\n",
    "        \n",
    "        return self.y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fea2888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid (object):\n",
    "    def forward(self, input):\n",
    "        self.x = 1/(1 + math.e**(-input))\n",
    "        return self.x\n",
    "    def backward(self, gradwrtoutput):\n",
    "        self.output = (self.x*(1-self.x))*gradwrtoutput\n",
    "        return self.output\n",
    "    def param(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "014e1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU (object):\n",
    "    def forward(self, input):\n",
    "        self.relu = input > 0\n",
    "        self.x = self.relu*input\n",
    "        return self.x\n",
    "    def backward(self, gradwrtoutput):\n",
    "        self.output = (self.x > 0)\n",
    "        self.output = self.output *  gradwrtoutput\n",
    "        return self.output\n",
    "    def param(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23b5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE (object):\n",
    "    def forward(self, predictions, targets):\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        self.mse = (((self.predictions - self.targets)**2).mean())\n",
    "        return self.mse\n",
    "    def backward(self, gradwrtoutput):\n",
    "        self.output = 2*((self.predictions - self.targets).mean())\n",
    "        self.output = self.output\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3197242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(object):\n",
    "    def __init__(self, factor_size):\n",
    "        self.kernel = torch.ones(factor_size,factor_size)\n",
    "        #print(self.kernel.shape)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.b, self.channels, self.s1, self.s2 = x.shape\n",
    "        self.s3, self.s4 = self.kernel.shape\n",
    "        x = x.reshape(self.b, self.channels, self.s1, 1, self.s2, 1)\n",
    "        self.kernel = self.kernel.reshape(1, self.s3, 1, self.s4)\n",
    "        return (x * self.kernel).reshape(self.b, self.channels, self.s1 * self.s3,self.s2 * self.s4) \n",
    "    \n",
    "    def backward(self,gradwrtoutput):\n",
    "        dL_dS = gradwrtoutput\n",
    "        dS_dX = self.kernel # [1, K, 1, K]\n",
    "        \n",
    "        #backward x\n",
    "        dL_dS_reshape = dL_dS.reshape(self.b, self.channels, self.s1, self.s3, self.s2, self.s4) #[B, C, SI, K, SI, K]\n",
    "        #print('dL_dS_reshape',dL_dS_reshape.shape)\n",
    "        \n",
    "        dL_dX = dL_dS_reshape /(dS_dX)\n",
    "        \n",
    "        #print(dL_dX.shape)\n",
    "        dL_dX = dL_dX.permute(0,1,2,4,3,5)\n",
    "        #print(dL_dX.shape)\n",
    "        \n",
    "        dL_dX_red = dL_dX[:,:,:,:,0,0]\n",
    "        #print(dL_dX_red.shape)\n",
    "    \n",
    "        #print(dL_dX.reshape(self.b, self.channels, self.s1, self.s2).shape)\n",
    "        \n",
    "        return dL_dX_red\n",
    "    \n",
    "    def param(self):\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9639adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(object):\n",
    "    def __init__(self, channels_input, channels_output, kernel_size, stride):\n",
    "        \n",
    "        self.weight = torch.empty(channels_output, channels_input, kernel_size, kernel_size).normal_()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.channels_output = channels_output\n",
    "        self.channels_input = channels_input\n",
    "        \n",
    "        self.grad = torch.empty(channels_output, channels_input, kernel_size, kernel_size)\n",
    "\n",
    "        #print('weight',self.weight.shape)\n",
    "         \n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        \n",
    "        _,_,H,W = imgs.shape\n",
    "        #print('h w ',H,W)\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.Hout = int((H - self.kernel_size)/self.stride + 1)\n",
    "        self.Wout = int((H - self.kernel_size)/self.stride + 1)\n",
    "        #print('Hout Wout', self.Hout, self.Wout)\n",
    "        self.x = imgs\n",
    "        #print('x', self.x.shape)\n",
    "        self.x_unfolded = unfold(self.x, kernel_size = (self.kernel_size, self.kernel_size), stride = self.stride)\n",
    "        #print('x unfold',self.x_unfolded.shape)\n",
    "        #print('w shape', self.weight.view(self.channels_output, -1).shape)\n",
    "        self.y = self.x_unfolded.transpose(1, 2).matmul(self.weight.view(self.channels_output, -1).t()).transpose(1, 2)\n",
    "        #print('y',self.y.shape)\n",
    "        self.y = fold(self.y, (int(self.Hout), int(self.Wout)),(1,1), stride = 1)\n",
    "        #self.y = self.y.view(1,10,15,15)\n",
    "        return self.y  #, self_x, self_weight\n",
    "    \n",
    "\n",
    "    def backward(self,gradwrtoutput):\n",
    "        dL_dS = gradwrtoutput # [B, O, SO, SO]\n",
    "        dS_dX = self.weight   # weight.shape [O, I, K, K]\n",
    "        #print('dL_dS', dL_dS.shape)\n",
    "        #print('dS_dX', dS_dX.shape)\n",
    "        \n",
    "        #define the size IxKxK\n",
    "        inKerKer_size = self.channels_input*self.kernel_size*self.kernel_size\n",
    "        #print('inKerKer_size',inKerKer_size)\n",
    "        \n",
    "        dL_dS_reshape = dL_dS.reshape(1,self.channels_output,-1) # [B, O, (SOxSO)]\n",
    "        #print('dL_dS_reshape',dL_dS_reshape.shape)\n",
    "        dS_dX_reshape = dS_dX.reshape(self.channels_output, -1).transpose(0,1)   # [O, (IxKxK)]^T\n",
    "        #print('dS_dX_reshape',dS_dX_reshape.shape)\n",
    "        \n",
    "        \n",
    "        #backward input\n",
    "        dL_dX_reshape = dS_dX_reshape @ dL_dS_reshape # [B, (IxKxK), (SOxSO)]\n",
    "        #print('dL_dX_reshape',dL_dX_reshape.shape)\n",
    "        dL_dX = fold(dL_dX_reshape, kernel_size = (self.kernel_size, self.kernel_size), stride = self.stride, output_size = (self.W, self.H)) # [B, I, SI, SI]\n",
    "        #print('dL_dX',dL_dX.shape)\n",
    "        \n",
    "        \n",
    "        #backward weight\n",
    "        dL_dS_reshape2 = dL_dS.reshape(self.channels_output, -1) # [O, (BxSOxSO)]\n",
    "        dS_dW = self.x_unfolded # [B, (IxKxK), (SOxSO)]\n",
    "        dS_dW_reshape = dS_dW.reshape(-1, inKerKer_size) # [(BxSOxSO), (IxKxK))] \n",
    "        dL_dW_reshape = dL_dS_reshape2 @ dS_dW_reshape # [O, (IxKxK)]\n",
    "        dL_dW = dL_dW_reshape.view(self.channels_output,  self.channels_input, self.kernel_size, self.kernel_size) # [O, I, K, K] \n",
    "        \n",
    "        self.grad = dL_dW\n",
    "        #print('backward weight', self.grad.shape )\n",
    "        #print('original weight', self.weight.shape )\n",
    "        #backward bias\n",
    "       \n",
    "        \n",
    "        return dL_dX  \n",
    "    \n",
    "    def param(self):\n",
    "        #print('hello convolution')\n",
    "        return self.weight, self.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b6544748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d14c6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential (object):\n",
    "    def __init__(self, *input):\n",
    "        self.layers = input\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for lay in self.layers:\n",
    "            print(lay)\n",
    "            x = lay.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradwrtoutput):\n",
    "        out = gradwrtoutput\n",
    "        for lay in reversed(self.layers):\n",
    "            print(lay)\n",
    "            out = lay.backward(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a473bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD (object):\n",
    "    def __init__(self,layers, lr):\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        print('zero grad')\n",
    "        #print(self.layers.layers)\n",
    "        \n",
    "        for lay in self.layers.layers:\n",
    "            print(lay)\n",
    "            values = lay.param()\n",
    "            if lay.param() is not None:\n",
    "                lay.grad = lay.grad.zero_()\n",
    "          \n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        for lay in self.layers.layers:\n",
    "            print(lay)\n",
    "            values = lay.param()\n",
    "            if lay.param() is not None:\n",
    "                #print('step', lay.weight)\n",
    "                #print('step lr', self.lr*lay.grad )\n",
    "                lay.weight = lay.weight + self.lr*lay.grad\n",
    "                #print('step', lay.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5a066237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Convolution object at 0x000001932FAC5550>\n",
      "<__main__.ReLU object at 0x000001932FABD310>\n",
      "<__main__.Upsample object at 0x000001932FABD0A0>\n",
      "<__main__.Sigmoid object at 0x000001932FAD3310>\n",
      "x torch.Size([1, 3, 32, 32])\n",
      "loss torch.Size([])\n",
      "zero grad\n",
      "<__main__.Convolution object at 0x000001932FAC5550>\n",
      "<__main__.ReLU object at 0x000001932FABD310>\n",
      "<__main__.Upsample object at 0x000001932FABD0A0>\n",
      "<__main__.Sigmoid object at 0x000001932FAD3310>\n",
      "<__main__.Sigmoid object at 0x000001932FAD3310>\n",
      "<__main__.Upsample object at 0x000001932FABD0A0>\n",
      "<__main__.ReLU object at 0x000001932FABD310>\n",
      "<__main__.Convolution object at 0x000001932FAC5550>\n",
      "out img torch.Size([1, 3, 32, 32])\n",
      "<__main__.Convolution object at 0x000001932FAC5550>\n",
      "<__main__.ReLU object at 0x000001932FABD310>\n",
      "<__main__.Upsample object at 0x000001932FABD0A0>\n",
      "<__main__.Sigmoid object at 0x000001932FAD3310>\n"
     ]
    }
   ],
   "source": [
    "conv1 = Convolution(3, 3, kernel_size = 2, stride = 2) # 1,3,16,16\n",
    "upsample = Upsample(2) # 1,3, 32, 32\n",
    "\n",
    "model = Sequential(Convolution(3, 3, kernel_size = 2, stride = 2),  # 1,3,16,16\n",
    "                   ReLU(),\n",
    "                   Upsample(2),\n",
    "                   Sigmoid()) # 1,3, 32, 32\n",
    "\n",
    "criterion = MSE()\n",
    "sigmoid = Sigmoid()\n",
    "optimizer = SGD(model, 0.005)\n",
    "\n",
    "\n",
    "img = torch.randn(1, 3, 32, 32)\n",
    "img_error = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "\n",
    "x = model.forward(img)\n",
    "\n",
    "print('x', x.shape)\n",
    "\n",
    "img = sigmoid.forward(x)\n",
    "\n",
    "\n",
    "loss = criterion.forward(img, img_error)\n",
    "print('loss', loss.shape)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = model.backward(img_error)\n",
    "\n",
    "print('out img', out_img.shape)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9422682c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Convolution object at 0x000001932CDE9E80>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-17fcdcb9ad01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mimg_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpredict_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-ee8ab7e4f7ee>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlay\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-52b8ebd87ca4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#print('x unfold',self.x_unfolded.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#print('w shape', self.weight.view(self.channels_output, -1).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_unfolded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannels_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m#print('y',self.y.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "criterion = MSE()\n",
    "\n",
    "autoencoder = Sequential(Convolution(3, 3, kernel_size = 2, stride = 2),\n",
    "                                    ReLU(),\n",
    "                                    Upsample(2),\n",
    "                                    Sigmoid())\n",
    "\n",
    "\n",
    "img = torch.randn(1, 3, 32, 32)\n",
    "img_error = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "predict_img = model.forward(img)\n",
    "print('prediction', predict_img.shape)\n",
    "loss = criterion.forward(predict_img, img_error)\n",
    "\n",
    "backward_img = model.backward(img)\n",
    "\n",
    "print('backward', backward_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9aee9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.sequence = Sequential(Convolution(3, 3, kernel_size = 2, stride = 2),\n",
    "                                    ReLU(),\n",
    "                                    Upsample(2),\n",
    "                                    Sigmoid())\n",
    "\n",
    "        self.criterion = MSE()\n",
    "        print('Model class initialisation')\n",
    "\n",
    "    def train(self):\n",
    "    \n",
    "        train_input = torch.randn(1, 3, 32, 32)\n",
    "        train_target = torch.randn(1, 3, 32, 32)\n",
    "    \n",
    "        predict_img = self.sequence.forward(train_input)\n",
    "        loss = self.criterion.forward(predict_img, train_target)\n",
    "\n",
    "        backward_img = self.sequence.backward(img)\n",
    "\n",
    "        return backward_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b601694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class initialisation\n",
      "<__main__.Convolution object at 0x000001932CDCAAF0>\n",
      "<__main__.ReLU object at 0x000001932CDE9D60>\n",
      "<__main__.Upsample object at 0x000001932CDE9880>\n",
      "<__main__.Sigmoid object at 0x000001932CDE9B20>\n",
      "<__main__.Sigmoid object at 0x000001932CDE9B20>\n",
      "<__main__.Upsample object at 0x000001932CDE9880>\n",
      "<__main__.ReLU object at 0x000001932CDE9D60>\n",
      "<__main__.Convolution object at 0x000001932CDCAAF0>\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn(1, 3, 32, 32)\n",
    "img_error = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "output = model.train()\n",
    "\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "185f9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    def __init__(self):\n",
    "        self.a = 10\n",
    "    def train(self):\n",
    "        self.a = self.a +8\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ab90a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder()\n",
    "result = model.train()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7dc5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
