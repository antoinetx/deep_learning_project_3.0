{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67a7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch\n",
    "from torch import empty , cat , arange\n",
    "from torch . nn . functional import fold , unfold\n",
    "\n",
    "#to delete\n",
    "#from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8d8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)\n",
    "\n",
    "inp = torch.randn(1, 3, 10, 12)\n",
    "w = torch.randn(2, 3, 4, 5)\n",
    "inp_unf = torch.nn.functional.unfold(inp, (4, 5))\n",
    "out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\n",
    "out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d290546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   \\nclass ConvolutionTransposed(object):\\n    def __init__(self, channels_input, channels_output, kernel_size, stride):\\n        self.weight = torch.empty(channels_output, channels_input, kernel_size, kernel_size).normal_()\\n        \\n        self.channel_input = channels_input\\n        #print('input channels', self.channel_input)\\n        self.kernel_size = kernel_size\\n        self.stride = stride\\n        \\n        print('weight',self.weight.shape)\\n        \\n        \\n    def forward(self, imgs):\\n        #print('forward')\\n        _,_,H,W = imgs.shape\\n        H_out = (H - 1)*self.stride + self.kernel_size \\n        W_out = (W - 1)*self.stride + self.kernel_size \\n        \\n        #print('Hout Wout', H_out, W_out)\\n        \\n        self.x = imgs.permute(1, 2, 3, 0).reshape(self.channel_input, -1)\\n        #print('x',self.x.shape)\\n        self.y = (self.weight.reshape(self.channel_input, -1)).t().matmul(self.x)\\n        #print('y',self.y.shape)\\n        self.y = self.y.reshape(self.y.shape[0], -1, imgs.shape[0])\\n        #print('y2',self.y.shape)\\n        self.y = self.y.permute(2, 0, 1)\\n        \\n        #print(self.y.shape)\\n        self.y = fold( self.y, (H_out, W_out), kernel_size=(self.kernel_size,self.kernel_size), stride=self.stride)\\n        \\n        return self.y\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"   \n",
    "class ConvolutionTransposed(object):\n",
    "    def __init__(self, channels_input, channels_output, kernel_size, stride):\n",
    "        self.weight = torch.empty(channels_output, channels_input, kernel_size, kernel_size).normal_()\n",
    "        \n",
    "        self.channel_input = channels_input\n",
    "        #print('input channels', self.channel_input)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        print('weight',self.weight.shape)\n",
    "        \n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        #print('forward')\n",
    "        _,_,H,W = imgs.shape\n",
    "        H_out = (H - 1)*self.stride + self.kernel_size \n",
    "        W_out = (W - 1)*self.stride + self.kernel_size \n",
    "        \n",
    "        #print('Hout Wout', H_out, W_out)\n",
    "        \n",
    "        self.x = imgs.permute(1, 2, 3, 0).reshape(self.channel_input, -1)\n",
    "        #print('x',self.x.shape)\n",
    "        self.y = (self.weight.reshape(self.channel_input, -1)).t().matmul(self.x)\n",
    "        #print('y',self.y.shape)\n",
    "        self.y = self.y.reshape(self.y.shape[0], -1, imgs.shape[0])\n",
    "        #print('y2',self.y.shape)\n",
    "        self.y = self.y.permute(2, 0, 1)\n",
    "        \n",
    "        #print(self.y.shape)\n",
    "        self.y = fold( self.y, (H_out, W_out), kernel_size=(self.kernel_size,self.kernel_size), stride=self.stride)\n",
    "        \n",
    "        return self.y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62afb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionTransposed(object):\n",
    "    def __init__(self,channels_input, channels_output, kernel_size, stride):\n",
    "        self.device = torch.device (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.weight = torch.empty(channels_output, channels_input, kernel_size, kernel_size).normal_()\n",
    "        self.grad = torch.empty(channels_output, channels_input, kernel_size, kernel_size)\n",
    "        \n",
    "        self.bias = 0\n",
    "        self.grad_bias = 0\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.channels_output = channels_output\n",
    "        self.channels_input = channels_input\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        B,I,SI,SI = x.shape\n",
    "        SO = (SI -1)*self.stride + self.kernel_size\n",
    "        self.H = SI\n",
    "        self.W = SI\n",
    "        self.x = x\n",
    "        self.x_reshape = self.x.reshape(B,I,-1) # [B,I,SI,SI]\n",
    "        self.weight_reshape = self.weight.permute(1,0,2,3).reshape(I,-1) # [I,(OxKxK)]\n",
    "        \n",
    "        self.y_reshape = self.weight_reshape.T @ self.x_reshape # [B, OxKxK, SIxSI]\n",
    "        print('y', self.y_reshape.shape)\n",
    "        \n",
    "        self.y = fold(self.y_reshape, kernel_size =(self.kernel_size,self.kernel_size), stride = self.stride, output_size=(SO,SO))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    def backward(self,gradwrtoutput):\n",
    "        \n",
    "        dL_dS = gradwrtoutput # [B, O, SO, SO]\n",
    "        print('dL_dS', dL_dS.shape)\n",
    "        dS_dX = self.weight # [O,I,K,K]\n",
    "        \n",
    "        #backward Input\n",
    "        dL_dS_unfold = unfold(dL_dS, kernel_size =(self.kernel_size,self.kernel_size), stride = self.stride)#[B, OxKxK, SIxSI]\n",
    "        print('unfold', dL_dS_unfold.shape )\n",
    "       \n",
    "        dS_dX_reshape = dS_dX.reshape(self.channels_input,-1) # [I,OxKxK]\n",
    "        print('dS_dX_reshape', dS_dX_reshape.shape)\n",
    "        \n",
    "        dL_dX_reshape = dS_dX_reshape @ dL_dS_unfold # [B,I,SIxSI]\n",
    "        dL_dX =  dL_dX_reshape.view(1,self.channels_input,self.H, self.W)\n",
    "        \n",
    "        #backward weight\n",
    "        dL_dS_unfold2 = dL_dS_unfold.permute(1,0,2).reshape(self.channels_input*self.kernel_size*self.kernel_size,-1) #[OxKxK, BxSIxSI]\n",
    "        print('unfold', dL_dS_unfold2.shape )\n",
    "        dS_dW_reshape = self.x.reshape(self.channels_input, -1 ) #[I, BxSIxSI]\n",
    "        \n",
    "        dL_dW_reshape =  dS_dW_reshape @ dL_dS_unfold2.T # [I, OxKxK]\n",
    "        \n",
    "        dL_dW = dL_dW_reshape.reshape(self.channels_input, self.channels_output, self.kernel_size, self.kernel_size )\n",
    "        \n",
    "        self.grad = dL_dW\n",
    "        print('self.grad',self.grad.shape)\n",
    "        \n",
    "        return dL_dX\n",
    "    \n",
    "    def param(self):\n",
    "        return self.weight, self.grad, self.bias, self.grad_bias \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b601694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y torch.Size([1, 108, 196])\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn(1, 3, 14,14)\n",
    "img_error = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "convT1 = ConvolutionTransposed(3,3,6,2)\n",
    "\n",
    "out = convT1.forward(img)\n",
    "\n",
    "print(out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb7dc5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_dS torch.Size([1, 3, 32, 32])\n",
      "unfold torch.Size([1, 108, 196])\n",
      "dS_dX_reshape torch.Size([3, 108])\n",
      "unfold torch.Size([108, 196])\n",
      "self.grad torch.Size([3, 3, 6, 6])\n",
      "torch.Size([1, 3, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "y = convT1.backward(out)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307fc51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
